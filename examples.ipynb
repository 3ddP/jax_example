{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "12c9f44b",
      "metadata": {
        "id": "12c9f44b"
      },
      "source": [
        "# Using jax and optax for fast math & automatic differentiation\n",
        "\n",
        "Today we'll be talking about jax and optax, some nifty libraries for the sort of stuff we deal with in signal processing. To begin, note that you'll have to get the following libraries:\n",
        "\n",
        "* jaxlib (this is just a dependency)\n",
        "* jax\n",
        "* optax\n",
        "\n",
        "These aren't natively available in windows. You can easily install them with pip or conda in linux, I'm not sure how exactly to do it in mac, but it should be doable. You can also use these libraries on google colab if you don't want to or can't install the libs. Colab environments have jaxlib and jax installed by default, so you can just go ahead and install optax and that'll be all.\n",
        "\n",
        "You can read the docs here https://jax.readthedocs.io/en/latest/jax-101/index.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfgUOQYJMd21",
        "outputId": "a583caa4-06bd-410e-c84b-a216ee867765"
      },
      "id": "YfgUOQYJMd21",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.2-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from optax) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from optax) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from optax) (1.0.0)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.8)\n",
            "Collecting chex>=0.0.4\n",
            "  Downloading chex-0.1.3-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 668 kB/s \n",
            "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.7+cuda11.cudnn805)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.7.1->optax) (1.15.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.3 optax-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "673b0e2e",
      "metadata": {
        "id": "673b0e2e"
      },
      "source": [
        "## jax\n",
        "jax is a library made by some google people, but it's not officially a google product. It provides functionality that is (almost) identical to that of numpy, which is nice. This means we don't have to learn how to do the math in a different way than we're used to. There are a few differences, though:\n",
        "* arrays are called \"device arrays\". This is because they can be stored in memory, GPU memory, or TPU memory.\n",
        "* from the previous point, you might have guessed it already: your code can run on CPU or GPU without any changes.\n",
        "* jax offers **just-in-time compilation (jit)**. This means that, when a block of code is executed, it will take **longer than usual** when it runs the first time. After that, calling the same block again can be 100x to 1000x faster.\n",
        "* jax offers **automatic differentiation**, which makes dealing with gradients, jacobians, and hessians much easier. \n",
        "* jit and auto diff are **composable**. Under some conditions, one can arbitrarily compose jit compilation with differentiation of different kinds. This means you can have compiled higher order derivatives.\n",
        "\n",
        "The cost to be paid is the following:\n",
        "* device arrays are **static**. They don't like being modified, unless you replace them completely.\n",
        "* moving a device array from one kind of storage to another is **costly**. For example, if we have a GPU device array and want to cast it back to a numpy array, the contents have to be copied from GPU memory to CPU memory, which is **slow**.\n",
        "* to arbitrarily compose jit and differentiation, functions have to be **well-behaved**. This is close, but not really as strict as **functional programming**. Things like if-else branching, while loops, modifying global variables, printing are forbidden and will yield disgustingly long error tracebacks. Intermediate variables and replacement thereof is fine.\n",
        "* the PRNG is **explicit**, meaning it does not keep track of its own state. **You** have to keep track of it!\n",
        "\n",
        "You can read more about these things here https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html\n",
        "\n",
        "Let's look at some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fc61f1aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc61f1aa",
        "outputId": "0591f7e4-22e5-4016-bc09-f3f097b1f0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with numpy:\n",
            "[-0.54487066  0.60026832]\n",
            "[-1.12534171 -0.81696422]\n",
            "[-0.40539631 -0.05918759]\n",
            "with jax:\n",
            "[ 1.81608667 -0.75488484]\n",
            "[ 1.81608667 -0.75488484]\n",
            "[ 1.81608667 -0.75488484]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#first, some jax config\n",
        "from jax.config import config\n",
        "#this let's us use double precision numbers. the default is single precision.\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "import jax #jax itself, in case we need some random stuff from it\n",
        "from jax import jit as jjit #this is a decorator for just-in-time compilation\n",
        "import jax.numpy as jnp #jax's version of numpy\n",
        "\n",
        "\n",
        "\n",
        "#lets print out some random vectors with numpy\n",
        "print('with numpy:')\n",
        "for i in range(3):\n",
        "    print(np.random.normal(size=(2,)))\n",
        "\n",
        "#in jax, we need to first generate keys that track the state\n",
        "seed = 0 #prng seed\n",
        "key = jax.random.PRNGKey(seed)\n",
        "print('with jax:')\n",
        "for i in range(3):\n",
        "    print(jax.random.normal(key, shape=(2,)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0676751",
      "metadata": {
        "id": "d0676751"
      },
      "source": [
        "As you can see, the PRNG is not updated automatically. We have to explicitly ask jax to generate new keys from the old one. We won't go into details, just know that the commong approach is to generate two keys from an old one. One of the keys is used for whatever we need it (e.g. sampling a random distribution). The other key is used to generate new keys in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dec427b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dec427b3",
        "outputId": "e9617f64-0785-401a-bc37-634594745192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attempt #2 with jax:\n",
            "[ 1.13787844 -0.14331426]\n",
            "[-0.06607245  0.79278191]\n",
            "[-2.20514421 -1.41387334]\n"
          ]
        }
      ],
      "source": [
        "seed = 0 #prng seed\n",
        "key = jax.random.PRNGKey(seed)\n",
        "print('attempt #2 with jax:')\n",
        "for i in range(3):\n",
        "    key, subkey = jax.random.split(key) #generate 2 new keys\n",
        "    print(jax.random.normal(subkey, shape=(2,))) #use one of them in our normal distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b652d3bc",
      "metadata": {
        "id": "b652d3bc"
      },
      "source": [
        "There we go! Keep in mind the results will always be the same if you do the splitting and generate the seed in exactly the same way. One could instead generate the seed with numpy's stateful PRNG to spice things up. If you prefer, you can simply generate random quantities in numpy and cast them to jax device arrays later. Just keep in mind this might be slower.\n",
        "\n",
        "---\n",
        "\n",
        "## automatic differentiation\n",
        "\n",
        "At any rate, that was a quick intro to what is arguably the weirdest part to deal with. The rest should be a lot more familiar. Let's set up a linear model and play around with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9bfa9cc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bfa9cc8",
        "outputId": "4d824f00-1135-492b-93ec-d8c8c80698fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.06660449  0.29026105  4.14155173 -0.11927005 -3.25627238  3.31087758\n",
            " -0.33040366 -4.40455618 -2.4080758  -3.53746502]\n"
          ]
        }
      ],
      "source": [
        "key, sk1, sk2 = jax.random.split(key, num = 3) #this time we'll make 3 random key. 2 for us to use, 1 for later.\n",
        "A = jax.random.normal(sk1, shape=(10,10)) #we make a random matrix...\n",
        "x = jax.random.normal(sk2, shape=(10,)) #... and a random vector...\n",
        "y = A.dot(x) #... and get their product as usual!\n",
        "print(y) #because why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea0b018",
      "metadata": {
        "id": "4ea0b018"
      },
      "source": [
        "So far, so good. Now, what if we want to do some gradient descent to estimate $\\boldsymbol{x}$ given $\\boldsymbol{A}$ and $\\boldsymbol{y}$? We'd need the gradient, for starters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e5ff61c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ff61c9",
        "outputId": "70be9b67-373e-4b0d-ae62-24db3701996b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "x_hat = jnp.zeros((10)) #same as with regular numpy\n",
        "\n",
        "#now, let's set up a cost function. least squares comes to mind.\n",
        "@jjit #note the decorator!\n",
        "def ls(x, A, y):\n",
        "    return jnp.sum((A.dot(x) - y)**2)\n",
        "\n",
        "#we already know what the gradient should be. how about we automate it, though?\n",
        "#grad takes in a function, and then argnums specifies the index of the parameters that the\n",
        "#gradient will be computed with respect to. here, we wanna differentiate wrt x. read the docs\n",
        "#for more info! https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html \n",
        "ls_grad = jjit(jax.grad(ls, argnums = 0)) #and let's jit it, too\n",
        "\n",
        "#let's compare the analytic gradient vs jax's result, shall we?\n",
        "key, subkey = jax.random.split(key)\n",
        "x_hat = jax.random.normal(subkey, shape=(10,))\n",
        "\n",
        "grad_reference = 2*A.T.dot(A.dot(x_hat) - y) #don't forget the 2 in front!\n",
        "grad_jax = ls_grad(x_hat, A, y) #note!! the gradient takes in the SAME parameters as the original function!\n",
        "print(grad_reference - grad_jax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53993c1c",
      "metadata": {
        "id": "53993c1c"
      },
      "source": [
        "Pretty cool, huh? Now let's set this up for the case in which we have complex quantities. This would make the cost function non holomorphic, and the complex derivatives won't exist. However, we instead use Wirtinger (or $\\mathbb{CR}-$) calculus. This considers complex parameters as two real parameters each, and takes only real derivatives. You can brush up your Wirtinger calc here https://arxiv.org/pdf/0906.4835.pdf; we'll just go ahead and use the result that tells us that, for real-valued functions of complex parameters, we can compute the gradient by differentiating w.r.t. the complex conjugate of the complex parameters. There's a sneaky factor of 2 here and there that we'll have to account for, but we already did that above. Shouldn't be much of a problem at all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "054f8ce9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "054f8ce9",
        "outputId": "1c565619-a217-4646-991f-50f4f9dec367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8.18734618  4.87918144 12.14883054 16.22418151 20.35057549  7.69868882\n",
            "  5.56329827  6.12455628 17.06479612 26.7119277 ]\n"
          ]
        }
      ],
      "source": [
        "key, sk1, sk2, sk3, sk4 = jax.random.split(key, num = 5)\n",
        "A = jax.random.normal(sk1, shape=(10,10)) + 1j*jax.random.normal(sk2, shape=(10,10))\n",
        "x = jax.random.normal(sk3, shape=(10,)) + 1j*jax.random.normal(sk4, shape=(10,))\n",
        "y = A.dot(x) #... and get their product as usual!\n",
        "\n",
        "@jjit #note the decorator!\n",
        "def ls(x, A, y):\n",
        "    temp = A.dot(x) - y #see? no problem with making a few imperative statements here and there\n",
        "    #we could've also just used the old expression and added in a jnp.real() & jnp.imag()\n",
        "    return jnp.real(temp.conjugate().T.dot(temp))\n",
        "\n",
        "ls_grad = jjit(jax.grad(ls, argnums = 0)) #same as before\n",
        "\n",
        "#and we again compare the two\n",
        "key, subkey = jax.random.split(key)\n",
        "x_hat = jax.random.normal(subkey, shape=(10,))\n",
        "\n",
        "grad_reference = A.conjugate().T.dot(A.dot(x_hat) - y) #we normally don't put the factor 2 here...\n",
        "grad_jax = ls_grad(x_hat, A, y)/2 #aha, but now it pops up here. this depends on the author, but some\n",
        "#people like putting a factor 2 in front of the Wirtinger gradient, since the dimensions of the\n",
        "#matrices double when splitting into real and imag parts\n",
        "print(jnp.abs(grad_reference - grad_jax))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b280c8db",
      "metadata": {
        "id": "b280c8db"
      },
      "source": [
        "So, what went wrong here? Let's take a closer look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "536d4c59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "536d4c59",
        "outputId": "e46efdb5-2ce4-4c06-e4d6-2da69ea5386c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. -8.18734618j 0. -4.87918144j 0.+12.14883054j 0.+16.22418151j\n",
            " 0.-20.35057549j 0. -7.69868882j 0. +5.56329827j 0. -6.12455628j\n",
            " 0.-17.06479612j 0.+26.7119277j ]\n"
          ]
        }
      ],
      "source": [
        "print(grad_reference - grad_jax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0a50f8",
      "metadata": {
        "id": "bf0a50f8"
      },
      "source": [
        "Aha! The real parts are correct, but the jax.grad() function expects a function $f:\\mathbb{R} \\to \\mathbb{R}$ or $f:\\mathbb{C} \\to \\mathbb{C}$ that is holomorphic. Our function was neither: we had $f:\\mathbb{C} \\to \\mathbb{R}$, which is not holomorphic in general. We can get around this by using jax.jacfwd(), jax.vjp(), or separating real and imaginary parts (you can read it out here https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html, but honestly I find the authors' notation to be... not the clearest). Let's take another shot at it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5db4681b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5db4681b",
        "outputId": "a12f8d8b-0be4-4fb2-bca2-1b9d3a29ba69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the gradient is of type <class 'tuple'> and length 2\n",
            "each element of the gradient is of type <class 'jaxlib.xla_extension.DeviceArray'> and shape (10,)\n"
          ]
        }
      ],
      "source": [
        "x_hat = x_hat + 0j\n",
        "@jjit #note the decorator!\n",
        "def ls(xr, xi, A, y): #split real and imaginary parts of the input\n",
        "    temp = A.dot(xr + 1j*xi) - y #see? no problem with making a few imperative statements here and there\n",
        "    #we could've also just used the old expression and added in a jnp.real() & jnp.imag()\n",
        "    return jnp.real(temp.conjugate().T.dot(temp)) #we don't need the real here anymore\n",
        "\n",
        "ls_grad = jjit(jax.grad(ls, argnums = (0,1))) #the parameters work much in the same way as with grad\n",
        "\n",
        "grad_reference = A.conjugate().T.dot(A.dot(x_hat) - y) #we normally don't put the factor 2 here...\n",
        "grad_jax = ls_grad(jnp.real(x_hat), jnp.imag(x_hat), A, y) \n",
        "print(f'the gradient is of type {type(grad_jax)} and length {len(grad_jax)}')\n",
        "print(f'each element of the gradient is of type {type(grad_jax[0])} and shape {grad_jax[0].shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3edfe6ee",
      "metadata": {
        "id": "3edfe6ee"
      },
      "source": [
        "So, when we specify a tuple in argnums, we also get a tuple back. Good to know! Furthermore, each element of the tuple has the dimension you'd expect of the gradient. All right. Double the number of parameters passed to the function we're differentiating, but they're real instead of complex, so it should take up the same amount of memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d4044e31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4044e31",
        "outputId": "14cf630e-7699-4e8b-e2cc-ca1a03ffd3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n"
          ]
        }
      ],
      "source": [
        "print(grad_reference - (grad_jax[0] + 1j*grad_jax[1])/2) #aha, but now it pops up here. \n",
        "#this depends on the author, but some people like putting a factor 2 in front of the Wirtinger gradient,\n",
        "#since the dimensions of the matrices double when splitting into real and imag parts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2318794",
      "metadata": {
        "id": "f2318794"
      },
      "source": [
        "Sweet! We got this working right. Just keep in mind that if a function is vector-valued, you'll have to use some form of jacobian function instead of taking gradients. They work the same way: a tuple of parameter indices has to be specified for differentiation, and the output will be a tuple in that same order. One can also call grad(grad(...)) or jacfwd(jacfwd(...)), you get the idea. \n",
        "\n",
        "All right, so what else can we do with this?\n",
        "\n",
        "---\n",
        "\n",
        "## efficient vectorization/batching/parallelization\n",
        "\n",
        "Turns out jax can efficiently \"vectorize\" function inputs and outputs. By this, I mean that it can add extra axes to the inputs and then evaluate the function in parallel over those axes (independently of each other). In machine learning, this would be similar to processing a batch of data together (if you decide to add/average/etc. the gradients). This is done with jax.vmap(). Additionally, there's jax.pmap(), which we won't go over. However, this latter one allows the function to be parallelized over hardware, e.g. different processors or GPUs. You can think of this latter one as using dask or multiprocessing. \n",
        "\n",
        "Let's go back to vmap and play around for a bit, though. For starters, let's take our least squares function and make it so that we can apply it to several values of $\\boldsymbol{x}$ and $\\boldsymbol{y}$ simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2ce71ec0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ce71ec0",
        "outputId": "b8f7fe6f-3c50-4cc5-b3d3-c2fb9b9837c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 924.90071277  503.20575404  882.42142103 1335.29396885  758.99111762]\n",
            "924.9007127664681\n"
          ]
        }
      ],
      "source": [
        "#let's see how jax.vmap() works, then\n",
        "#with in_axes, we provide a tuple whose elements correspond to the input parameters of the function.\n",
        "#the parameters specify which axis of the corresponding parameter to vectorize over. here, we've told\n",
        "#jax that the axis 1 (columns) will represent different values of the same variable (i.e. each column\n",
        "#of the input is a new parameter x or y). furthermore, we've specified we want to do this for xr, xi, and y,\n",
        "#with no vectorization for the matrix A. out_axes works much in the same way. the original function returns\n",
        "#a scalar, so i've set out_axes to 0 to return a column vector. each entry in the column vector will correspond\n",
        "#to a column of the vectorized parameters.\n",
        "batch_ls = jjit(jax.vmap(ls, in_axes = (1, 1, None, 1), out_axes = 0))\n",
        "\n",
        "#let's do 5 evaluations of ls now\n",
        "num_batches = 5\n",
        "key, sk1, sk2, sk3, sk4, sk5, sk6 = jax.random.split(key, num = 7) #yeah, yeah... poor style\n",
        "A = jax.random.normal(sk1, shape=(10,10)) + 1j*jax.random.normal(sk2, shape=(10,10))\n",
        "x_batch = jax.random.normal(sk3, shape=(10,num_batches)) + 1j*jax.random.normal(sk4, shape=(10,num_batches))\n",
        "y_batch = A.dot(x_batch) #... and get their product as usual!\n",
        "#these ones are going to be used instead of the true x values to get nonzero errors\n",
        "x_hat_batch = jax.random.normal(sk5, shape=(10,num_batches)) + 1j*jax.random.normal(sk6, shape=(10,num_batches))\n",
        "\n",
        "#here are all the outputs\n",
        "ls_outputs = batch_ls(jnp.real(x_hat_batch), jnp.imag(x_hat_batch), A, y_batch)\n",
        "\n",
        "#and for comparison, we'll also compute the first one using the original ls function\n",
        "ls0 = ls(jnp.real(x_hat_batch[:,0]), jnp.imag(x_hat_batch[:,0]), A, y_batch[:,0])\n",
        "\n",
        "print(ls_outputs)\n",
        "print(ls0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bcc0cc5",
      "metadata": {
        "id": "8bcc0cc5"
      },
      "source": [
        "Awesomeness. In some scenarios, this can save you the heartbreak of dealing with block-diagonal matrices and/or loops, so it can replace einsum in some scenarios.\n",
        "\n",
        "---\n",
        "\n",
        "## batch gradient descent with optax\n",
        "\n",
        "All right, so now let's bring all of this together to do something cool. We can set up some batch gradient descent by using jax and optax, which should be useful for many of us. \n",
        "\n",
        "Let's set up a linear model for a polynomial coefficient estimation task (fun times with Vandermode matrices). Since we know the gradient and have a fixed model, we could just do everything analytically if we pick a fixed step size based on the singular values of the model matrix. That's boring stuff. **Let's use a data-driven approach to optimally learn the descent step size instead.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3e18dce3",
      "metadata": {
        "id": "3e18dce3"
      },
      "outputs": [],
      "source": [
        "#let's not make this unnecessarily painful, let's stick to low degree polys, e.g. quadratics. this means\n",
        "#there are 3 coefficients, and observations of the form y_i = ax_i^2 + bx_i + c can be vectorized so that\n",
        "#the coefficients c and y are related through a vandermonde matrix that depends on x. then, we can set up \n",
        "#a least squares loss as ||c - c_hat||_2^2.\n",
        "\n",
        "@jjit\n",
        "def loss(c, c_hat): #this tells us how good our estimate of c is\n",
        "    return jnp.sum((c - c_hat)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b65f6d7",
      "metadata": {
        "id": "1b65f6d7"
      },
      "source": [
        "Because of the built-in Vandermonde function's column order, this means the parameters in c must be arranged in decreasing order of the power of x they will multiply. Just something to keep in mind. Now, we already know what the gradient looks like for this. How about we set up a function that takes care of it? We'll pick a fixed number of iterations and estimate the step size. **This is equivalent to algorithm unfolding/unrolling, which is pretty hyped up in the ML community at the moment. Coolbeans.** This is a super simplified example, though, where we're just learning a single hyperparameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "51a7d2a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51a7d2a8",
        "outputId": "0a6999eb-19c0-484a-b551-a9d892980446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4  2  1]\n",
            " [ 9  3  1]\n",
            " [16  4  1]]\n",
            "note that the powers increase toward the left\n"
          ]
        }
      ],
      "source": [
        "#here's just a quick example of what vander does. we'll use it later\n",
        "x = jnp.array([2,3,4])\n",
        "print(jnp.vander(x,N=3))\n",
        "print('note that the powers increase toward the left')\n",
        "\n",
        "@jjit\n",
        "def estimate(A, y, c0, mus): #this one estimates c\n",
        "    #NOTE! another thing here. we could perfectly well do something like\n",
        "    '''\n",
        "    for mu in mus: #take a handful of iterations\n",
        "        c0 = c0 - mu*A.T.dot(A.dot(c0) - y) #do a gradient descent step\n",
        "    '''\n",
        "    #HOWEVER! this is takes a long time for jax to compile. \n",
        "    \n",
        "    #we can and SHOULD instead use jax loops:\n",
        "    def body(n, parameters):\n",
        "      A, y, c0, mus = parameters #unpack parameters\n",
        "      c0 = c0 - mus[n]*A.T.dot(A.dot(c0) - y) #compute a descent step with mu[n]\n",
        "      return (A, y, c0, mus)\n",
        "\n",
        "    #this function takes in a range for n (the first two params),\n",
        "    #a body function with parameters n and \"val\", and returns a updated \"val\",\n",
        "    #and an initial value for the variable \"val\"\n",
        "    _, _, c0, mus = jax.lax.fori_loop(0, len(mus), body, (A, y, c0 ,mus))\n",
        "    \n",
        "    return c0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a65bcf88",
      "metadata": {
        "id": "a65bcf88"
      },
      "source": [
        "Now comes the cool part. We can plug in our estimator into the loss function, and then have jax auto diff w.r.t. the step size without batting an eyelash."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ceb9bfe2",
      "metadata": {
        "id": "ceb9bfe2"
      },
      "outputs": [],
      "source": [
        "@jjit\n",
        "def target(A, y, c, c0, mu):\n",
        "    c0 = estimate(A, y, c0, mu)\n",
        "    return loss(c, c0)\n",
        "\n",
        "grad = jjit(jax.grad(target, argnums = 4)) #differentiate w.r.t. step sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e34dde2",
      "metadata": {
        "id": "1e34dde2"
      },
      "source": [
        "You have to admit, that's pretty nice and succinct. It's also a full neural network with as many layers as the length of the vector of step sizes. Let's take it a step further and turn this into a batch gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "da919556",
      "metadata": {
        "id": "da919556"
      },
      "outputs": [],
      "source": [
        "batch_grad = jjit(jax.vmap(grad, in_axes=(None,1,1,None,None), out_axes=1)) #vectorize over x, y, and c, and put the\n",
        "#per example gradients as columns in the output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5aea6f2",
      "metadata": {
        "id": "d5aea6f2"
      },
      "source": [
        "Let's try out the gradient function and see what the output looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ce99c3d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce99c3d6",
        "outputId": "78a2ba19-5f25-44ba-da2c-5dd92a08f2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 5)\n"
          ]
        }
      ],
      "source": [
        "mu = jnp.ones(50)*1e-7 #completely made up value for the step size\n",
        "\n",
        "#let's generate some data\n",
        "key, sk1, sk2 = jax.random.split(key, num = 3)\n",
        "nb = 5 #number of batches\n",
        "x = jnp.linspace(-10,10,20)\n",
        "A = jnp.vander(x, N = 3)\n",
        "c_batches = jax.random.normal(sk2, shape=(3, nb)) #3 coefficients for each of the 5 batches\n",
        "y_batches = jnp.vander(x, N = 3).dot(c_batches) #20 x 5\n",
        "\n",
        "per_example_grad = batch_grad(A, y_batches, c_batches, jnp.zeros((3)), mu)\n",
        "print((per_example_grad.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17aa3450",
      "metadata": {
        "id": "17aa3450"
      },
      "source": [
        "Excellent, now all the pieces are in place. We have a cost function, we know how to compute its gradient w.r.t. the parameters of interest, and we can even do so for a batch of several examples at the same time. Time for optax to make its entrace. Optax is a library that originally was a part of jax, but eventually it became a thing of its own under the DeepMind team. The basic functionality is pretty simple:\n",
        "* choose an optimizer from the ones optax offers (you can also write your own, but we won't)\n",
        "* initialize it with the initial guess of the parameters. this sets the state of the optimizer\n",
        "* use the optimizer's update(gradient, state) function to update its state and compute an update to the parameters based on the gradient (recall that many optimization techniques take the gradient as a starting point, and then go on to compute momentum, estimate the Hessian, etc.)\n",
        "* apply the update to the parameters\n",
        "and that's about it. In code, this takes one line per step. It also allows arbitrary nesting of parameter structures, which is the reason the 4th step exists. It can save you the pain of figuring out how exactly to apply the update to a list of dictionaries of pytrees of matrices, for example.\n",
        "\n",
        "Let's see how this works in code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c13843fb",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c13843fb",
        "outputId": "0f00b75f-6426-403d-c27e-5997a3ca5a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "currently in batch 1 from 1000...\n",
            "currently in batch 51 from 1000...\n",
            "currently in batch 101 from 1000...\n",
            "currently in batch 151 from 1000...\n",
            "currently in batch 201 from 1000...\n",
            "currently in batch 251 from 1000...\n",
            "currently in batch 301 from 1000...\n",
            "currently in batch 351 from 1000...\n",
            "currently in batch 401 from 1000...\n",
            "currently in batch 451 from 1000...\n",
            "currently in batch 501 from 1000...\n",
            "currently in batch 551 from 1000...\n",
            "currently in batch 601 from 1000...\n",
            "currently in batch 651 from 1000...\n",
            "currently in batch 701 from 1000...\n",
            "currently in batch 751 from 1000...\n",
            "currently in batch 801 from 1000...\n",
            "currently in batch 851 from 1000...\n",
            "currently in batch 901 from 1000...\n",
            "currently in batch 951 from 1000...\n"
          ]
        }
      ],
      "source": [
        "import optax\n",
        "\n",
        "#recall we already made an initial guess for the parameters, we called it mu\n",
        "mu = jnp.zeros(800)\n",
        "\n",
        "#let's go with the adam optimizer, because why not? the learning rate i chose here is also arbitrary.\n",
        "#it could also be optimized by wrapping all of this stuff into yet another function, and optimizing it\n",
        "#with optax! but we won't :P\n",
        "learning_rate = 1e-6\n",
        "optimizer = optax.rmsprop(learning_rate) #choose an optimizer\n",
        "opt_state = optimizer.init(mu) #initialize the optimizer state by giving it the initial guess\n",
        "\n",
        "#now, we set up a training scenario. let's optimize the descent step sizes a large number\n",
        "#of examples, which we will split into smaller batches\n",
        "total = 1000000\n",
        "batch_size = 1000\n",
        "num_batches = int(total/batch_size)\n",
        "x_len = 20 #let's say we have 20 samples for each polynomial\n",
        "x = jnp.linspace(-5, 5, x_len)\n",
        "A = jnp.vander(x, N = 3)\n",
        "for i in range(num_batches):\n",
        "    if i%50 == 0:\n",
        "        print(f'currently in batch {i+1} from {num_batches}...')\n",
        "    #let's generate some data\n",
        "    key, sk1, sk2 = jax.random.split(key, num = 3)\n",
        "    c_batches = jax.random.uniform(sk2, shape=(3, batch_size),\n",
        "                minval = -100, maxval = 100) #3 x batch_size\n",
        "    y_batches = A.dot(c_batches) #x_len x batch_size\n",
        "    gradients = batch_grad(A, y_batches, c_batches, jnp.zeros((3)), mu)\n",
        "    \n",
        "    #let's go ahead and average the gradients. we could also just add them up, or otherwise produce\n",
        "    #a weighted sum, depending on how we want to look at the cost function.\n",
        "    gradients = jnp.sum(gradients, axis=1)/batch_size #average over the columns\n",
        "    \n",
        "    #now, let's use the gradients to produce a parameter update and ALSO update the optimizer state\n",
        "    updates, opt_state = optimizer.update(gradients, opt_state)\n",
        "    \n",
        "    #and finally, apply the update to our parameters\n",
        "    mu = optax.apply_updates(mu, updates)\n",
        "    mu = mu*(mu > 0) #projected gradient\n",
        "    #print(gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb3ece9",
      "metadata": {
        "id": "0fb3ece9"
      },
      "source": [
        "Let's check out what we got!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "02b13c78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "02b13c78",
        "outputId": "73ef652d-96df-48fd-8a12-26e1f158db05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the largest possible step size should be 0.0006546662390544673\n",
            "we get 0.0006511752533547151 with our overkill estimation setup\n",
            "the original coefficients were [-42.0778063   12.80168182 -20.70545477]\n",
            "our estimator found[-42.04822179  12.80168182 -21.30791232]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbf621840d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.plot(mu)\n",
        "\n",
        "#and now, let's test the step sizes on a polynomial\n",
        "key, sk1 = jax.random.split(key)\n",
        "c  = jax.random.uniform(sk1, shape=(3,), minval = -100, maxval = 100)\n",
        "y = A.dot(c)\n",
        "\n",
        "eigs,_ = jnp.linalg.eigh(A.T.dot(A))\n",
        "print(f'the largest possible step size should be {2/eigs[-1]}')\n",
        "print(f'we get {mu[0]} with our overkill estimation setup')\n",
        "\n",
        "key, sk1 = jax.random.split(key)\n",
        "c_hat = jax.random.uniform(sk1, shape=(3,), minval = -100, maxval = 100)\n",
        "c_hat = estimate(A, y, c_hat, mu)\n",
        "\n",
        "print(f'the original coefficients were {c}')\n",
        "print(f'our estimator found{c_hat}')\n",
        "plt.figure()\n",
        "plt.plot(x,y)\n",
        "plt.plot(x, A.dot(c_hat))\n",
        "plt.legend(('original', 'estimated'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there we go! Super unnecessary in this example, but the idea was to get you familiar and comfortable with composition of jax functions.\n",
        "\n",
        "I'd again like to highlight that this last example could be seen either as algorithm unfolding, or alternatively as hyper parameter optimization.\n",
        "\n",
        "As a final word of caution, recall that we nested several layers of jit, grad, and functions of our own making. You WILL find bugs, and the error stacks WILL be disgusting. Be patient :P"
      ],
      "metadata": {
        "id": "KALm0_PfMjzo"
      },
      "id": "KALm0_PfMjzo"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l9vRfGyoNUtG"
      },
      "id": "l9vRfGyoNUtG",
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "examples.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}